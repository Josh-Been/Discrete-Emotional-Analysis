{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Emotional Model for Digital Scholarship\n",
    "\n",
    "![Baylor Libraries Banner](https://github.com/Josh-Been/Sentiment-Per-Line/blob/master/Capture.PNG?raw=true \"Baylor University Libraries\")\n",
    "\n",
    "This Jupyter Notebook will guide researchers through the process of applying Valence, Arousal, and Dominance scoring system to scholarly literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was developed by the Baylor University Libraries - Digital Scholarship Program\n",
    "\n",
    "http://blogs.baylor.edu/digitalscholarship/\n",
    "\n",
    "02/16/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Ensure Formic library is Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor='  >>  '\n",
    "print cursor, 'installing formic'\n",
    "# formic allows for simple subdirectory iterations\n",
    "!pip install formic\n",
    "# Import Python Libraries\n",
    "# All included in Pandas except where specified\n",
    "import os, string, formic     # formic not in pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Tkinter import *\n",
    "from tkFileDialog import askopenfilename\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "print '\\n', cursor, 'libraries fully loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, Browse for Zotero CSV Export & Visualize Counts Per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove all non-ascii characters\n",
    "def clean(item):\n",
    "    stripped = ' '.join([x.strip(string.punctuation) for x in item.split()])\n",
    "    stripped=stripped.replace('\\n','')\n",
    "    stripped=stripped.replace('\\r','')\n",
    "    stripped=stripped.replace(',','')\n",
    "    stripped = (c for c in stripped if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "# function to browse for file\n",
    "def browse_file():\n",
    "    root = Tk()\n",
    "    zotero=askopenfilename()\n",
    "#     path=os.path.dirname(zotero).replace('csv','files')\n",
    "    root.update()\n",
    "    root.destroy()\n",
    "    return zotero\n",
    "\n",
    "# function to write Google Graphs bar chart\n",
    "def bar_chart(d, name):\n",
    "    f=open(os.path.dirname(zotero).replace('csv','')+'bar.html','r')\n",
    "    web = f.read()\n",
    "    f.close()\n",
    "    html = web.split('// HERE')\n",
    "    f=open(os.path.dirname(zotero).replace('csv','')+name,'w')\n",
    "    f.write(html[0])\n",
    "    for k, v in sorted(d.items()):\n",
    "        f.write('[\\\"'+str(k)+'\\\", '+str(v)+'],')\n",
    "    f.write(html[1])\n",
    "    f.close()\n",
    "    return name\n",
    "    \n",
    "# Enable all fields to view\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read CSV to dataframe\n",
    "zotero=browse_file()\n",
    "df = pd.read_csv(zotero)\n",
    "\n",
    "# Add field totalling documents per year to dataframe\n",
    "df['Year_Counts'] = df.groupby(['Publication Year'])['Key'].transform('count')\n",
    "\n",
    "# Create dictionary of counts per year\n",
    "year_counts = {}\n",
    "for i, row in df.iterrows():\n",
    "    year_counts[int(row['Publication Year'])]=int(row['Year_Counts'])\n",
    "\n",
    "# Create visualization\n",
    "IFrame(bar_chart(year_counts,'bar_years.html'), width=1100, height=700) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third, Browse for Stop Words, Load Publication Text into Memory & Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_stop(document,stopwords):\n",
    "    stops=[]\n",
    "    stops[:]=[]\n",
    "    for line in stopwords:\n",
    "        stops.append(line)\n",
    "    words = [word for word in document.lower().split() if (not word in stops and not word.isdigit() and not 'http' in word and len(word)>3)]\n",
    "    return words\n",
    "\n",
    "# Browse for stop word list\n",
    "stopwords = browse_file()\n",
    "\n",
    "# Create new field in dataframe\n",
    "df['full_text']=''\n",
    "\n",
    "# Create (and clear, to be safe) new lists\n",
    "full_text=[]\n",
    "full_text[:]=[]\n",
    "\n",
    "pre_cr_text=[]\n",
    "pre_cr_text[:]=[]\n",
    "\n",
    "pre_sbc_text=[]\n",
    "pre_sbc_text[:]=[]\n",
    "\n",
    "post_sbc_text=[]\n",
    "post_sbc_text[:]=[]\n",
    "\n",
    "# Path to files directory containing the text files\n",
    "path=os.path.dirname(zotero).replace('csv','files')\n",
    "print cursor, 'Reading full text into memory'\n",
    "i=0\n",
    "fileset = formic.FileSet(include='**/*.txt', directory=path)\n",
    "for text in fileset:\n",
    "    i+=1\n",
    "    f=open(text,'r')\n",
    "    for row in df['File Attachments']:\n",
    "        if clean(os.path.basename(text).replace('.txt','.pdf')) in clean(row):\n",
    "            ftext=f.read()\n",
    "            cleaned = clean(ftext)\n",
    "            full_text.append(remove_stop(cleaned,stopwords))\n",
    "            if df['Publication Year'][i-1] <= 1970:\n",
    "                pre_cr_text.append(remove_stop(cleaned,stopwords))\n",
    "            elif df['Publication Year'][i-1] <= 1990:\n",
    "                pre_sbc_text.append(remove_stop(cleaned,stopwords))\n",
    "            else:\n",
    "                post_sbc_text.append(remove_stop(cleaned,stopwords))\n",
    "            print cursor, i, '/', len(df)\n",
    "            if i%50==0:\n",
    "                clear_output(wait=True)\n",
    "                print cursor, 'Reading full text into memory'\n",
    "            break\n",
    "f.close()\n",
    "\n",
    "all_words = ' '.join(str(r) for v in full_text for r in v)\n",
    "pre_cr_text_words = ' '.join(str(r) for v in pre_cr_text for r in v)\n",
    "pre_sbc_text_words = ' '.join(str(r) for v in pre_sbc_text for r in v)\n",
    "post_sbc_text_words = ' '.join(str(r) for v in post_sbc_text for r in v)\n",
    "\n",
    "print '\\n', cursor, 'Building Word Cloud'\n",
    "df['full_text'] = full_text\n",
    "\n",
    "print '\\n', cursor, 'Entire Corpus'\n",
    "print cursor, len(full_text), 'documents'\n",
    "plt.rcParams[\"figure.figsize\"] = [25,15]\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "wordcloud = WordCloud().generate(all_words)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print '\\n', cursor, '1970 and Earlier (before effects of Civil Rights Movement)'\n",
    "print cursor, len(pre_cr_text), 'documents'\n",
    "wordcloud = WordCloud().generate(pre_cr_text_words)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print '\\n', cursor, '1980 - 1990 (After Civil Rights Movement - Before SBC Resolution)'\n",
    "print cursor, len(pre_sbc_text), 'documents'\n",
    "wordcloud = WordCloud().generate(pre_sbc_text_words)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print '\\n', cursor, '1991 - 2000 (After SBC Resolution)'\n",
    "print cursor, len(post_sbc_text), 'documents'\n",
    "wordcloud = WordCloud().generate(post_sbc_text_words)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth, Enter Words and Phrases to Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(b):\n",
    "    clear_output()\n",
    "    phrase_search[:]=[]\n",
    "    display(text)\n",
    "    display(button)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    phrase_search.append(text.value)\n",
    "    print text.value\n",
    "\n",
    "print 'Enter word or phrase and ENTER'\n",
    "phrase_search=[]\n",
    "phrase_search[:]=[]\n",
    "\n",
    "text=widgets.Text()\n",
    "display(text)\n",
    "text.on_submit(handle_submit)\n",
    "\n",
    "button=widgets.Button(description='CLEAR')\n",
    "display(button)\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth, Browse for Discrete Emotional Ratings & Emotional Bubble Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse for emotional discrete ratings list\n",
    "df_emotions = pd.read_csv(browse_file())\n",
    "\n",
    "measurables=[]\n",
    "measurables[:]=[]\n",
    "\n",
    "proximity_words=[]\n",
    "proximity_words[:]=[]\n",
    "\n",
    "aval=[]\n",
    "aval[:]=[]\n",
    "\n",
    "vval=[]\n",
    "vval[:]=[]\n",
    "\n",
    "dval=[]\n",
    "dval[:]=[]\n",
    "\n",
    "a_score=[]\n",
    "a_score[:]=[]\n",
    "\n",
    "v_score=[]\n",
    "v_score[:]=[]\n",
    "\n",
    "d_score=[]\n",
    "d_score[:]=[]\n",
    "\n",
    "p=1\n",
    "\n",
    "print cursor, 'Processing text surrounding search word(s)'\n",
    "\n",
    "for row in df['full_text']:\n",
    "    measurables[:]=[]\n",
    "    a_score[:]=[]\n",
    "    v_score[:]=[]\n",
    "    d_score[:]=[]\n",
    "    for word in phrase_search:        \n",
    "        if word in row:\n",
    "            for i in range(0,len(row)-1):\n",
    "                if len(row)>0:\n",
    "                    if word in row[i]:\n",
    "                        for a in range(1,4):\n",
    "                            try:\n",
    "                                measurables.append(row[i-a])     \n",
    "                                n=0\n",
    "                                for item in df_emotions['Word']:\n",
    "                                    if row[i-a] == item:\n",
    "                                        a_score.append(df_emotions['A.Mean.Sum'][n])\n",
    "                                        v_score.append(df_emotions['V.Mean.Sum'][n])\n",
    "                                        d_score.append(df_emotions['D.Mean.Sum'][n])\n",
    "                                        break\n",
    "                                    n+=1\n",
    "                            except:\n",
    "                                pass\n",
    "                            try:\n",
    "                                measurables.append(row[i+a])\n",
    "                                n=0\n",
    "                                for item in df_emotions['Word']:\n",
    "                                    if row[i+a] == item:\n",
    "                                        a_score.append(df_emotions['A.Mean.Sum'][n])\n",
    "                                        v_score.append(df_emotions['V.Mean.Sum'][n])\n",
    "                                        d_score.append(df_emotions['D.Mean.Sum'][n])\n",
    "                                        break\n",
    "                                    n+=1\n",
    "                            except:\n",
    "                                pass\n",
    "    print cursor, p, '/', len(df)\n",
    "    if p%50==0:\n",
    "        clear_output(wait=True)\n",
    "        print cursor, 'Processing text surrounding search word(s)'\n",
    "    p+=1\n",
    "    temp_list=measurables[:]\n",
    "    t_a=a_score[:]\n",
    "    t_v=v_score[:]\n",
    "    t_d=d_score[:]\n",
    "    a_temp=np.mean(t_a)\n",
    "    v_temp=np.mean(t_v)\n",
    "    d_temp=np.mean(t_d)\n",
    "    aval.append(a_temp)\n",
    "    vval.append(v_temp)\n",
    "    dval.append(d_temp)\n",
    "    proximity_words.append(temp_list)\n",
    "\n",
    "# print aval\n",
    "    \n",
    "df['proximity']=proximity_words\n",
    "df['a']=aval\n",
    "df['v']=vval\n",
    "df['d']=dval\n",
    "\n",
    "df_ratings=df.groupby('Publication Year').mean()\n",
    "\n",
    "f=open('bubble.html','r')\n",
    "fb=open('line.html','r')\n",
    "web=f.read()\n",
    "web_b=fb.read()\n",
    "f.close()\n",
    "fb.close()\n",
    "html = web.split('!!! HERE')\n",
    "htmlb = web_b.split('!!! HERE')\n",
    "f=open('bubble_emotions.html','w')\n",
    "fb=open('line_emotions.html','w')\n",
    "f.write(html[0])\n",
    "fb.write(htmlb[0])\n",
    "m=0\n",
    "for year in df_ratings['Date']:\n",
    "    if df_ratings['v'].iloc[m]>=0:\n",
    "        if int(year)<= 1970:\n",
    "            period = '1970 & earlier'\n",
    "        elif int(year)<=1990:\n",
    "            period = '1971 - 1990'\n",
    "        else:\n",
    "            period = '1991 - 2000'\n",
    "        f.write('[\\''+str(year)+'\\', '+str(df_ratings['v'].iloc[m])+', '+str(df_ratings['a'].iloc[m])+', \\''+period+'\\', '+str(df_ratings['d'].iloc[m])+'],\\n')\n",
    "        fb.write('[\\''+str(year)+'\\', '+str(df_ratings['v'].iloc[m])+', '+str(df_ratings['a'].iloc[m])+', '+str(df_ratings['d'].iloc[m])+'],\\n')\n",
    "    m+=1\n",
    "f.write(html[1])\n",
    "fb.write(htmlb[1])\n",
    "f.close()\n",
    "fb.close()\n",
    "\n",
    "# Create visualization\n",
    "IFrame('bubble_emotions.html', width=1100, height=700) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth, Create a Line Chart by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('line_emotions.html', width=1100, height=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
